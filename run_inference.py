import argparse
import os
import pickle
import shutil
import sys
import time
from functools import partial, reduce
from itertools import starmap
from multiprocessing import Pool
from subprocess import PIPE, Popen
from typing import Any, Callable, Dict, List, Tuple, Union, Iterable

import cv2
import numpy as np
import torch
import torch.nn.functional as F
import torchvision
from PIL import Image
from tqdm import tqdm
from UVTextureConverter import Atlas2Normal

from datasets import DeepFashion as DF
# from models import ANet, PNet
# from stylegan2.stylegan import ModelLoader


def run_densepose_network(source_img_dir: str, output_pkl: str,
                          dp_path: str=os.path.join('.', 'densepose'),
                          stdout: bool=True) -> int:
    """
    Run denspose on single image path or directory of images and produce a
    pickle file.

    Arguments:
        source_img_dir [str]: path to directory of imgs (or path to img -- bad!)
        output_pkl [str]: path to save output pickle
        dp_path [opt str=apply_net.py]: path to densepose folder
        stdout [opt bool=True]: whether or not to output densepose stdout

    Returns:
        n_files [int]: number of files densepose was run on

    Side Effects:
        Saves large pickle (~ 10mb / image) to given path
    """
    MODEL_URL = 'https://dl.fbaipublicfiles.com/densepose/' + \
                'densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl'

    apply_net_path = os.path.join(dp_path, 'apply_net.py')
    config_path = os.path.join(dp_path, 'configs',
                               f'{MODEL_URL.split("/")[-3]}.yaml')

    call =  ['python3', apply_net_path, 'dump', '-v']
    call += ['--output', output_pkl]
    call += [config_path]
    call += [MODEL_URL, source_img_dir]

    with Popen(call, stdout=PIPE, stderr=PIPE) as proc:
        if stdout:
            print(proc.stdout.read().decode('utf-8'))
        print(proc.stderr.read().decode('utf-8'))

    if os.path.isfile(source_img_dir):
        n_files = 1
    else:
        n_files = len((1 for n in os.scandir(source_img_dir)
                      if os.path.isfile(os.path.join(source_img_dir, n))))

    return n_files


def create_texture_atlas(IUV: torch.tensor,
                         XYZ: torch.tensor, N: int=200) -> torch.tensor:
    """
    Take in an IUV map generated by densepose, and a source image and produce
    a texture atlas that can be converted to a SURREAL texture map.

    Arguments:
        IUV [tensor; (3 x H x W)]: Posemap IUV coordinates
        XYZ [tensor; (H x W x 3)]: Original RGB Image tensor
        N   [optional int]: size of each texture segment (N x N x 3)

    Returns:
        Custom Atlas [tensor; (4N, 6N, 3)]: A texture atlas

    Side Effects:
        None
    Note: Densepose Default atlas is of shape (6N x 4N x 3), requires returned
    atlas to be transposed to apply to pose using apply_net.py
    """
    # 24 = (6 x 4) parts of size (N x N x [R, G, B])
    atlas = torch.zeros(4 * N, 6 * N, 3)
    I, U, V = IUV  # IUV(I in {0, ..., 24};  U, V in [0, 1]^{m \times n})
    parts = []

    for part_id in range(1, 25):
        part = torch.zeros(N, N, 3).long()  # Each part is N x N x [R, G, B]
        x, y = torch.where(IUV[0] == part_id)  # select pixels belonging to part {part_id}
        x_idx = (U[x, y] * (N - 1)).long()  # must be long to index into tensor
        y_idx = (1 - (V[x, y]) * (N - 1)).long()  # x ∝ U; y ∝ (1 - V)

        part[x_idx, y_idx] = XYZ[x, y, :]  # copy RGB pixels from image on [x, y] mask
        parts.append(part)

    # 4:6 (transpose of densepose default of 6:4)
    for i in range(6):
        for j in range(4):
            # inverse of ./densepose/vis/densepose_results_textures.py:get_texture()::line 58
            atlas[(N * j):N * (j + 1), (N * i):N * (i + 1)] = parts[6 * j + i]

    return atlas


def convert_atlas_to_SURREAL(atlas: torch.tensor) -> torch.tensor:
    """
    Convert a (transposed) denspose atlas format (4N x 6N; N=200) into an SMPL
    appearance map (512 x 512).

    Arguments:
        atlas [tensor; (800, 1200, 3)]: the (transposed) densepose atlas

    Returns:
        normal_tex [tensor; (512, 512, 3)]: the SMPL format appearance map

    Side Effects:
        Adds arbitrarily to the computation graph of atlas, detach if you plan
        on a backwards pass.
    """
    atlas_tex_stack = Atlas2Normal.split_atlas_tex(atlas.numpy())
    converter = Atlas2Normal(atlas_size=200, normal_size=512)
    normal_tex = converter.convert(atlas_tex_stack)
    return normal_tex


def generate_ipa(d: Dict[str, Any], Ts: Iterable[Callable], dests: Tuple[str]) -> Tuple[torch.Tensor]:
    """
    Create I, P, A from densepose vis dict.
    Arguments:
        d: densepose image dict vis

    Returns:
        IPA [Tuple[Tensor]; (I, P, A)]:
            image tensor (I), pose map tensor (P), appearance map tensor (A)
    
    Side EFfects:
        Loads source image from file path, will error if path no longer exists.
    """
    file_path = d['file_name']
    file_name = os.path.basename(file_path)

    if 'pred_densepose' not in d:
        return file_name, (None, None, None)

    I = torchvision.transforms.ToTensor()(Image.open(file_path)).permute(1, 2, 0)

    x1, y1, x2, y2 = d['pred_boxes_XYXY'][0]
    x_off, x_delta = int(x1), int(x2 - x1)
    y_off, y_delta = int(y1), int(y2 - y1)

    T = d['pred_densepose'][0]
    IUV = torch.cat((T.labels.unsqueeze(0), T.uv.clamp(0, 1)), dim=0)
    P = torch.zeros(I.shape)
    P[y_off:y_off + y_delta, x_off:x_off + x_delta] = IUV.permute(1, 2, 0)

    atlas = create_texture_atlas(P.permute(2, 0, 1).cpu(), (I * 255).long())
    A = torch.from_numpy(convert_atlas_to_SURREAL(atlas))

    I_path, P_path, A_path = dests
    # cv2.imwrite(I_path, (I * 255).cpu().numpy())
    # cv2.imwrite(P_path, (P * 255).cpu().numpy())
    # cv2.imwrite(A_path, (A * 255).cpu().numpy())

    torchvision.utils.save_image(I.permute(2, 0, 1), I_path)
    cv2.imwrite(P_path, (P * 255).cpu().numpy())
    torchvision.utils.save_image(A.permute(2, 0, 1), A_path)
    return file_name


def parse_args():
    """
    Returns user input command line args in expected order for main function.

    Arguments:
        None

    Returns:
        source [str]: path to src image or dir
        target [str]: path to targ image or dir
        outputs [str]: path to output dir
        densepose [str]: path to densepose folder from detectron2
        image_size [int]: image size to use in model forward
        batched [bool]: set to true if src/targ are directories
        batch_size [int]: set size for model forward batch if --batched is used
        v [bool]: set true if you want verbose output
    """
    parser = argparse.ArgumentParser(description='Styleposegan inference')
    parser.add_argument('--source', type=str, help='path to the source image(s)')
    parser.add_argument('--target', type=str, help='path to the target image(s)')
    parser.add_argument(
        '--outputs', type=str,
        default=os.path.join('.', 'data', 'inference_outputs'),
        help='directory to store outputs and temporary inference files'
    )
    parser.add_argument(
        '--scripted_model_path', type=str,
        default=os.path.join('.', 'checkpoints', f'scripted_model_1.0.0.pt'),
        help='path to model checkpoint directory'
    )
    parser.add_argument(
        '--densepose', type=str, default=os.path.join('.', 'densepose'),
        help='path to denspose folder'
    )
    parser.add_argument(
        '--image_size', type=int, default=[256, 256], nargs='+',
        help='image size to use in model forward'
    )
    parser.add_argument(
        '--batched', action='store_true',
        help='use if the input source/target paths are for directories'
    )
    parser.add_argument(
        '--batch_size', type=int, default=4,
        help='if batched, use batch_size for model forward'
    )
    parser.add_argument('-v', action='store_true', help='verbose output')
    parser.add_argument('--save_model', action='store_true', help='save torchscript model')
    parser.add_argument(
        '--load_from', type=int, default=-1,
        help='checkpoint number for model, use \'-1\' for latest'
    )
    parser.add_argument(
        '--model_dir', type=str,
        default=os.path.join('.', 'checkpoints', 'models', 'default'),
        help='Only if --save_model!, path to model checkpoint directory, ' + \
             'eg. \'./checkpoints/models/dev-fixes\' (not a path to a .pt file!)'
    )

    args = parser.parse_args()
    image_size = (tuple(args.image_size[:2]) if len(args.image_size) >= 2
                  else (*args.image_size, *args.image_size))
    model_dir, name = os.path.split(args.model_dir)
    base_dir = os.path.split(model_dir)[0]
    return (args.source, args.target, args.outputs, args.scripted_model_path,
            args.densepose, image_size, args.batched, args.batch_size, args.v,
            args.save_model, args.load_from, base_dir, name)


def main(src: str, targ: str,
         out_dir: str=os.path.join('.', 'data', 'inference_outputs'),
         model_path: str=os.path.join('.', 'checkpoints', f'scripted_model_1.0.0.pt'),
         densepose_path: str=os.path.join('.', 'densepose'),
         image_size: Union[Tuple[int], int]=(256, 256),
         batched: bool=False, batch_size: int=4,
         verb: bool=False, save_model: bool=False, load_from: int=-1,
         base_dir: str=os.path.join('.', 'checkpoints'),
         model_name: str='default') -> None:
    """
    """
    curr_dir = os.getcwd()  # keep track of current user directory
    os.chdir(densepose_path)  # change to denspose dir for imports
    sys.path.append(densepose_path)  # Add densepose to path to evaluate on

    if batched:
        print('batched inference is not supported yet')
        exit()
        assert os.path.isdir(src), f'Expected {src} to be a directory'
        assert os.path.isdir(targ), f'Expected {targ} to be a directory'

        exp_targ_names = (os.path.join(targ, d.name) for d in os.scandir(src))
        assert all(map(os.path.isfile, exp_targ_names)), \
               'Expected source and target dirs to have matching file names'
    else:
        assert os.path.isfile(src), f'Expected {src} to be a file'
        assert os.path.isfile(targ), f'Expected {targ} to be a file'
        verb and print('batch_size automatically set to 1')
        batch_size = 1

    if not os.path.isdir(out_dir):
        verb and print('output directory doesn\'t exist yet, creating now...')
        os.mkdir(out_dir)

    results_dir = os.path.join(out_dir, 'results')
    if not os.path.isdir(results_dir):
        verb and print('creating results dir {results_dir}')
        os.mkdir(results_dir)

    src_pkl = os.path.join(out_dir, 'src.pkl')
    targ_pkl = os.path.join(out_dir, 'targ.pkl')

    verb and print('computing pose maps')
    n_src_files = run_densepose_network(src, src_pkl, densepose_path, verb)
    n_targ_files = run_densepose_network(targ, targ_pkl, densepose_path, verb)

    assert n_src_files == n_targ_files, 'src/targ had different # of files'

    with open(src_pkl, 'rb') as f1, open(targ_pkl, 'rb') as f2:
        verb and print('loading densepose results')
        src_data, targ_data = pickle.load(f1), pickle.load(f2)

    scale_and_crop = DF.scale_and_crop(image_size)
    T = DF.get_transforms(scale_and_crop, is_tensor=False)
    to_tensor = torchvision.transforms.ToTensor()

    data_map = {'src': src_data, 'targ': targ_data}
    data_pairs = dict() # {img_name: {'src' (I, P, A), 'targ': (I, P, A)}}
    make_paths = lambda s: (
        os.path.join(results_dir, f'I_{s}.jpg'),
        os.path.join(results_dir, f'P_{s}.jpg'),
        os.path.join(results_dir, f'A_{s}.jpg')
    )
    if not batched:
        ds = src_data[0]
        dt = targ_data[0]
        dests_s = make_paths('s')
        dests_t = make_paths('t')
        file_name = generate_ipa(ds, T, dests_s)
        _ = generate_ipa(dt, T, dests_t)

        # if not all(t is not None for t in ipa_s) or \
        #        all(t is not None for t in ipa_t):
        #     print(f'Error computing densepose for one of the files, exiting')
        #     exit()

        ipa_s = (t(x) for t, x in zip(T, map(Image.open, dests_s)))
        ipa_t = (t(x) for t, x in zip(T, map(Image.open, dests_t)))
        
        data_pairs[file_name] = (ipa_s, ipa_t)
    else:
        print('batched operation not supported yet')
        exit()
    
    empty = (None, None)
    batch_size = min(batch_size, n_src_files)

    # model_dir, model_name = os.path.split(model_dir)
    if save_model or not os.path.isfile(model_path):
        verb and print(f'saving torchscript jit model to {model_path}')
        from stylegan2 import Trainer
        model = Trainer(
            name=model_name,
            base_dir=base_dir,
            image_size=image_size[0]
        )
        model.load(load_from)
        model = model.GAN.eval()
        pair = next(iter(data_pairs.values()), empty)
        (I_s, P_s, A_s), (I_t, P_t, _) = pair

        I_s = I_s.unsqueeze(0).cuda()#, size=image_size)
        P_s = P_s.unsqueeze(0).cuda()#, size=image_size)
        A_s = A_s.unsqueeze(0).cuda()#, size=image_size)
        I_t = I_t.unsqueeze(0).cuda()#, size=image_size)
        P_t = P_t.unsqueeze(0).cuda()#, size=image_size)
        sample_input = ((I_s, P_s, A_s), (I_t, P_t))
        scripted_model = torch.jit.trace(model, sample_input)
        scripted_model.save(model_path)

    verb and print(f'loading latest scripted model from {model_path}')
    # model = ModelLoader(base_dir=os.path.split(model_dir)[0], name=model_name,
    #                     batch_size=batch_size, image_size=image_size[0])
    model = torch.jit.load(model_path)
    

    empty = (None, None)
    data_iter = iter(data_pairs.items())
    img, pair = next(data_iter, empty)
    while img is not None and pair is not None:
        (I_s, P_s, A_s), (I_t, P_t, _) = pair

        I_s = I_s.unsqueeze(0).cuda()#, size=image_size)
        P_s = P_s.unsqueeze(0).cuda()#, size=image_size)
        A_s = A_s.unsqueeze(0).cuda()#, size=image_size)
        I_t = I_t.unsqueeze(0).cuda()#, size=image_size)
        P_t = P_t.unsqueeze(0).cuda()#, size=image_size)

        (image_size, I_dash_s, I_dash_s_to_t, I_dash_s_ema,
         I_dash_s_to_t_ema) = model((I_s, P_s, A_s), (I_t, P_t))

        A_s = F.interpolate(A_s, size=image_size)

        regular = torch.cat(
            (I_s, P_s, A_s, I_t, P_t, I_dash_s, I_dash_s_to_t), dim=0
        )

        ema = torch.cat(
            (I_s, P_s, A_s, I_t, P_t, I_dash_s_ema, I_dash_s_to_t_ema), dim=0
        )

        # final_img = torch.cat((regular, ema), dim=0)  # to concat reg + ema together
        reg_save_path = os.path.join(results_dir, f'{img}_inference.jpg')
        ema_save_path = os.path.join(results_dir, f'{img}_inference_EMA.jpg')

        torchvision.utils.save_image(regular, reg_save_path, nrow=batch_size)
        torchvision.utils.save_image(ema, ema_save_path, nrow=batch_size)
        img, pair = next(data_iter, empty)
    
    # go back to previous directory
    os.chdir(curr_dir)


if __name__ == '__main__':
    main(*parse_args())
